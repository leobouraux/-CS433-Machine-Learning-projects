{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "392cOSqPFERU"
   },
   "source": [
    "### Load Colab Notebook settings\n",
    "\n",
    "**Link Google Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "lCzJUkcCEl_v",
    "outputId": "4e69b6a9-dbfd-415b-b1a3-1b618b3ec5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "root_path = 'gdrive/My Drive/'  #change dir to your project folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hdl6pIq8FQPC"
   },
   "source": [
    "**Try GPU power**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "id": "MUsW6qc9EZb4",
    "outputId": "6f0670aa-889b-46df-986b-77ce4f00eec6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "pzfBKZdjEqmi",
    "outputId": "2c7e6e3d-c4d9-438c-ca08-2f57e61bcb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 12.4 GB  | Proc size: 1.4 GB\n",
      "GPU RAM Free: 11372MB | Used: 69MB | Util   1% | Total 11441MB\n"
     ]
    }
   ],
   "source": [
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isn’t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzS0iDgAEuE3"
   },
   "outputs": [],
   "source": [
    "#!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGFnbrKgDKgK"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "24oyvhn4F3ZP"
   },
   "outputs": [],
   "source": [
    "# mettre dans un .py qaund on a trouvé les bons params\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.image as mpimg\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as keras\n",
    "\n",
    "def data_load(directory_name): \n",
    "    filenames = os.listdir(directory_name)\n",
    "    imgs = []\n",
    "    for i, fileNb in enumerate(filenames):\n",
    "        full_name = directory_name+fileNb\n",
    "        img=mpimg.imread(full_name)\n",
    "        imgr = img_to_array(img)\n",
    "        imgs.append(imgr)\n",
    "        sys.stdout.write(\"\\rImage {}/{} is being loaded\".format(i+1,len(filenames)))\n",
    "        sys.stdout.flush()    \n",
    "    return np.asarray(imgs)\n",
    "\n",
    "def dataGenerator(batch_size, data, labels, aug_dict, image_save_prefix =\"image\", mask_save_prefix=\"mask\", seed=1):\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "\n",
    "    image_generator = image_datagen.flow(\n",
    "        data,\n",
    "        batch_size = batch_size,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow(\n",
    "        labels,\n",
    "        batch_size = batch_size,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "    \n",
    "    for (img,mask) in zip(image_generator, mask_generator):\n",
    "        if(np.max(img) > 1):\n",
    "          img = img / 255\n",
    "          mask = mask /255\n",
    "          mask[mask > 0.5] = 1\n",
    "          mask[mask <= 0.5] = 0\n",
    "        yield (img,mask)\n",
    "\n",
    "def uneti(input_size, pretrained_weights=None, verbose=True):\n",
    "    ##Parameters\n",
    "    model = Sequential()\n",
    "    kernel_size = (3,3)\n",
    "    pool_size = (2,2)\n",
    "    alpha_relu = 0.1\n",
    "    regularizer = 1e-6\n",
    "\n",
    "    #Size of input matrix\n",
    "    #To change according to the shape\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    #Add convolution \n",
    "    model.add(Convolution2D(64,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    model.add(Convolution2D(64,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "\n",
    "    model.add(Convolution2D(128,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    model.add(Convolution2D(128,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "\n",
    "    model.add(Convolution2D(256,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "    model.add(MaxPooling2D(pool_size))\n",
    "    model.add(Convolution2D(256,\n",
    "                            kernel_size,\n",
    "                            padding='same',\n",
    "                            input_shape=input_size))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(LeakyReLU(alpha_relu))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    \n",
    "    adam_optimizer = Adam(lr=0.0005)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    if(verbose == True):\n",
    "        model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \t  model.load_weights(pretrained_weights)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WnmnJnLqlgpS"
   },
   "source": [
    "**1. Load the data and split it into validation and training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_xJRCGK3lZJZ",
    "outputId": "360ba49c-788b-447d-dfdd-237750dedad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 100/100 is being loaded"
     ]
    }
   ],
   "source": [
    "imgs = data_load(root_path+'data_rs/train/images/')\n",
    "gts = data_load(root_path+'data_rs/train/groundtruth/')\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "X_train, X_test, gt_train, gt_test = train_test_split(imgs, gts, test_size=TEST_SIZE, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lvi9n98BlnwO"
   },
   "source": [
    "**2. Data augmentation and generators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aNGfqPnIDKgM"
   },
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=0.15,\n",
    "                    width_shift_range=0.1,\n",
    "                    height_shift_range=0.1,\n",
    "                    shear_range=0.1,\n",
    "                    zoom_range=0.1,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "train_generator = dataGenerator(32, X_train, gt_train, data_gen_args)\n",
    "validation_generator = dataGenerator(32, X_test, gt_test, data_gen_args)\n",
    "\n",
    "\n",
    "## LE PROBLÈME EST DANS LA FACON DE GÉNÉNER DES IMAGES: EUX ILS UTILISENT\n",
    "## CE QUI EST DANS LA CELLULE DU BAS\n",
    "\n",
    "# ca c'est juste le code utilisé dans notre version dans dataGenerator\n",
    "image_generator = image_datagen.flow(\n",
    "        data,\n",
    "        batch_size = batch_size,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    mask_generator = mask_datagen.flow(\n",
    "        labels,\n",
    "        batch_size = batch_size,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UE-NduubDy3n"
   },
   "outputs": [],
   "source": [
    "#utilisation de la fonction\n",
    "train_generator = image_generator(self.train_data_split,\n",
    "                                          self.train_label_split,\n",
    "                                          self.WINDOW_SIZE,\n",
    "                                          batch_size = 32, \n",
    "                                          upsample=True)\n",
    "        \n",
    "validation_generator = image_generator(self.validation_data_split,\n",
    "                                          self.validation_label_split,\n",
    "                                          self.WINDOW_SIZE,\n",
    "                                          batch_size = 32,\n",
    "                                          upsample=True)\n",
    "\n",
    "\n",
    "# déclaration de la fonction\n",
    "def image_generator(images, ground_truths, window_size, batch_size = 64,\n",
    "                    upsample=False):\n",
    "    np.random.seed(0)\n",
    "    imgWidth = images[0].shape[0]\n",
    "    imgHeight = images[0].shape[1]\n",
    "    half_patch = constants.IMG_PATCH_SIZE // 2\n",
    "    \n",
    "    padSize = (window_size - constants.IMG_PATCH_SIZE) // 2 #42\n",
    "    paddedImages = []\n",
    "    for image in images:\n",
    "        paddedImages.append(pad_image(image,padSize))\n",
    "        \n",
    "    while True:\n",
    "        batch_input = []\n",
    "        batch_output = [] \n",
    "        \n",
    "        #rotates the whole batch for better performance\n",
    "        randomIndex = np.random.randint(0, len(images))  \n",
    "        img = paddedImages[randomIndex]\n",
    "        gt = ground_truths[randomIndex]\n",
    "        \n",
    "        # rotate with probability 10 / 100\n",
    "        random_rotation = 0\n",
    "        if (np.random.randint(0, 100) < 10):\n",
    "            rotations = [90, 180, 270, 45, 135, 225, 315]\n",
    "            random_rotation = np.random.randint(0, 7)\n",
    "            img = rot(img, np.array([imgWidth+2*padSize, imgHeight+2*padSize]), rotations[random_rotation])\n",
    "            gt = rot(gt, np.array([imgWidth, imgHeight]), rotations[random_rotation]) \n",
    "        \n",
    "        background_count = 0\n",
    "        road_count = 0\n",
    "        while len(batch_input) < batch_size:\n",
    "            x = np.empty((window_size, window_size, 3))\n",
    "            y = np.empty((window_size, window_size, 3))\n",
    "            \n",
    "            \n",
    "            # we need to limit possible centers to avoid having a window in an interpolated part of the image\n",
    "            # we limit ourselves to a square of width 1/sqrt(2) smaller\n",
    "            if(random_rotation > 2):\n",
    "                boundary = int((imgWidth - imgWidth / np.sqrt(2)) / 2)\n",
    "            else:\n",
    "                boundary = 0\n",
    "            center_x = np.random.randint(half_patch + boundary, imgWidth  - half_patch - boundary)\n",
    "            center_y = np.random.randint(half_patch + boundary, imgHeight - half_patch - boundary)\n",
    "            \n",
    "            x = img[center_x - half_patch:center_x + half_patch + 2 * padSize,\n",
    "                    center_y  - half_patch:center_y + half_patch + 2 * padSize]\n",
    "            y = gt[center_x - half_patch : center_x + half_patch,\n",
    "                   center_y - half_patch : center_y + half_patch]\n",
    "            \n",
    "            # vertical\n",
    "            if(np.random.randint(0, 2)):\n",
    "                x = np.flipud(x)\n",
    "            \n",
    "            # horizontal\n",
    "            if(np.random.randint(0, 2)):\n",
    "                x = np.fliplr(x)\n",
    "            \n",
    "            label = [0., 1.] if (np.array([np.mean(y)]) >  constants.FOREGROUND_THRESHOLD) else [1., 0.]\n",
    "            \n",
    "            # makes sure we have an even distribution of road and non road if we oversample\n",
    "            if not upsample:\n",
    "                batch_input.append(x)\n",
    "                batch_output.append(label)\n",
    "            elif label == [1.,0.]:\n",
    "                # case background\n",
    "                background_count += 1\n",
    "                if background_count != batch_size // 2:\n",
    "                    batch_input.append(x)\n",
    "                    batch_output.append(label)\n",
    "            elif label == [0.,1.]:\n",
    "                # case road\n",
    "                road_count += 1\n",
    "                if road_count != batch_size // 2:\n",
    "                    batch_input.append(x)\n",
    "                    batch_output.append(label)\n",
    "                \n",
    "        batch_x = np.array( batch_input )\n",
    "        batch_y = np.array( batch_output )\n",
    "\n",
    "        yield( batch_x, batch_y )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b9wOFiLqDKgQ"
   },
   "source": [
    "i = 0\n",
    "for (img,mask) in myGene: \n",
    "    i = i+1\n",
    "    print(i, img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1KQDTZvmT6y"
   },
   "source": [
    "**3. Load the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "UFbB8DjFDKgW",
    "outputId": "4bb78f1a-8730-49cc-d652-f80516b3a5d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 100, 100, 64)      1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 50, 50, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 25, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 25, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 4610      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,150,018\n",
      "Trainable params: 1,150,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = uneti(input_size = (100,100,3), pretrained_weights=None, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHXl6nSKmtxo"
   },
   "source": [
    "**4. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "colab_type": "code",
    "id": "n-M8DQyPDKgb",
    "outputId": "e6cbda74-fd88-4521-d2bf-f1fbc2ec11b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7895482d53d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     validation_steps = 300)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_13_input to have shape (100, 100, 3) but got array with shape (400, 400, 3)"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=700,\n",
    "                    epochs=30, \n",
    "                    verbose=1,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nBo7QWxPxBb"
   },
   "source": [
    "### Save model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RLZdB07aPveI",
    "outputId": "32f98111-084a-4c04-b9d2-c94d63dd7d3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "OUTPUT_FILENAME = \"Model_UNET_400\"\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(OUTPUT_FILENAME + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(OUTPUT_FILENAME + \".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qobnMaMXlUty"
   },
   "source": [
    "### Predictions (don't run it on Colab)\n",
    "\n",
    "**Load model** [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HQrnXQy6jkK3"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('simple_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuEiBnDzjxfk"
   },
   "source": [
    "**Resize images in a folder** [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A6xx_o74V6gp"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "path1 = \"##UNET/data/membrane/tmp/\"\n",
    "path2 = \"##UNET/data/membrane/test/\"\n",
    "\n",
    "filenames = os.listdir(path1)\n",
    "for i, fileNb in enumerate(filenames):\n",
    "    if(fileNb!='.DS_Store'):        \n",
    "        im1 = Image.open(path1+fileNb)\n",
    "        side = 400\n",
    "        # use one of these filter options to resize the image\n",
    "        im2 = im1.resize((side, side), Image.NEAREST)\n",
    "        im2.save(path2+fileNb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeI81fYrV6gv"
   },
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import skimage.transform as trans\n",
    "\n",
    "\n",
    "def testGeneratorNEW(batch_size, train_path, image_folder, aug_dict, image_save_prefix =\"prediction\",\n",
    "                     num_class=2, save_to_dir=None, target_size=(256,256), seed=1):\n",
    "    image_datagen = ImageDataGenerator(**aug_dict)\n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "    for img in image_generator:\n",
    "        img = img / 255\n",
    "        yield img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngVayB-4V6gx"
   },
   "outputs": [],
   "source": [
    "def model_UNETOP(input_size = (256,256,3, 1), pretrained_weights=None):\n",
    "    inputs = Input(shape=(256,256,3))\n",
    "\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge6 = concatenate([drop4,up6], axis = 3)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv1,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "    conv10 = Conv2D(3, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "    \n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    if(pretrained_weights):\n",
    "    \tmodel.load_weights(pretrained_weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--GDB9-ojDIB"
   },
   "outputs": [],
   "source": [
    "from Model_UNET import *\n",
    "\n",
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "test_generatorNEW = testGeneratorNEW(2, 'UNET/data/membrane','test',data_gen_args,save_to_dir = None)\n",
    "\n",
    "#testGene = testGenerator(\"UNET/data/membrane/test\")\n",
    "\n",
    "\n",
    "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "#\n",
    "#testGene = test_datagen.flow_from_directory(\n",
    "#        \"UNET/data/membrane/test/\",\n",
    "#        target_size=(400, 400),\n",
    "#        color_mode=\"rgb\",\n",
    "#        shuffle = False,\n",
    "#        class_mode='categorical',\n",
    "#        batch_size=1)\n",
    "\n",
    "print('1')\n",
    "\n",
    "model = model_UNETOP()\n",
    "print('2')\n",
    "\n",
    "model.load_weights(\"Model_UNET.h5\")\n",
    "print('3')\n",
    "\n",
    "results = model.predict_generator(test_generatorNEW, 30,verbose=1)\n",
    "print('4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rhBD3io_jDko"
   },
   "outputs": [],
   "source": [
    "def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
    "    for i,item in enumerate(npyfile):\n",
    "        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
    "        io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tIJSQWtxjDnj"
   },
   "outputs": [],
   "source": [
    "saveResult(\"UNET/data/membrane/test\",results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "trainDeep.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
