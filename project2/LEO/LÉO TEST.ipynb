{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import model_from_json\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.randint(0, 10000)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "TEST_SIZE = 50\n",
    "IMG_PATCH_SIZE = 16 # image size should be an integer multiple of this number!\n",
    "NUM_CHANNELS = 3 # RGB images\n",
    "FOREGROUND_THRESHOLD = 0.25  # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 100\n",
    "WINDOW_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 100/100 is being processedShuffle data\n",
      "Image 100/100 is being processedShuffle data\n"
     ]
    }
   ],
   "source": [
    "def data_augmentation(directory_name): \n",
    "    datagen = ImageDataGenerator()\n",
    "    filenames = os.listdir(directory_name)\n",
    "    \n",
    "    #create 24 rotated images for one image\n",
    "    angls = np.arange(0, 360, 15)\n",
    "    zooms = np.array([1., 0.85, 0.8, 0.75, 0.8, 0.85,\n",
    "                      1., 0.85, 0.8, 0.75, 0.8, 0.85,\n",
    "                      1., 0.85, 0.8, 0.75, 0.8, 0.85,\n",
    "                      1., 0.85, 0.8, 0.75, 0.8, 0.85])\n",
    "    imgs = []\n",
    "    \n",
    "    for i, fileNb in enumerate(filenames):\n",
    "        img=mpimg.imread('../Datasets/training/images/'+fileNb)\n",
    "        imgr = img_to_array(img)\n",
    "        for j, angle in enumerate(angls):\n",
    "            zoom = zooms[j]\n",
    "            img2 = datagen.apply_transform(x=imgr, transform_parameters={'theta':angle, 'zx':zoom, 'zy':zoom})\n",
    "            imgs.append(img2)\n",
    "\n",
    "        sys.stdout.write(\"\\rImage {}/{} is being processed\".format(i+1,len(filenames)))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    print(' ... Shuffle data ...')\n",
    "    imgs1 = np.asarray(imgs)\n",
    "    np.random.seed(seed)\n",
    "    rand = np.random.randint(imgs1.shape[0], size=imgs1.shape[0])\n",
    "    return imgs1[rand, :, :, :]\n",
    "\n",
    "tr_imgs = data_augmentation(\"../Datasets/training/images/\")      #100 images --> 2400 images\n",
    "gt_imgs = data_augmentation(\"../Datasets/training/groundtruth/\") #100 images --> 2400 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# add model layers\n",
    "model.add(Conv2D(16, kernel_size=IMG_PATCH_SIZE, activation='relu',\n",
    "                      input_shape=(IMG_PATCH_SIZE, IMG_PATCH_SIZE, NUM_CHANNELS)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile model using accuracy to measure model performance\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    df = np.sum(v)\n",
    "    if df > FOREGROUND_THRESHOLD:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "def img_crop(im, w, h):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if is_2d:\n",
    "                im_patch = im[j:j+w, i:i+h]\n",
    "            else:\n",
    "                im_patch = im[j:j+w, i:i+h, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(gt_imgs):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"            \n",
    "    imgs = gt_imgs[:TRAINING_SIZE, :, :, :]\n",
    "\n",
    "    num_images = len(imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE) for i in range(num_images)]\n",
    "    data = np.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = np.asarray([value_to_class(np.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(np.float32)\n",
    "\n",
    "    \n",
    "def create_patches(im):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    for i in range(0, imgheight, IMG_PATCH_SIZE):\n",
    "        for j in range(0, imgwidth, IMG_PATCH_SIZE):\n",
    "            im_patch = im[j:j+IMG_PATCH_SIZE, i:i+IMG_PATCH_SIZE, :]\n",
    "            list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "\n",
    "def extract_data(tr_imgs):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"#todo\n",
    "    \n",
    "    imgs = tr_imgs[:TRAINING_SIZE, :, :, :]\n",
    "\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    N_PATCHES_PER_IMAGE = (IMG_WIDTH/IMG_PATCH_SIZE)*(IMG_HEIGHT/IMG_PATCH_SIZE)\n",
    "\n",
    "    img_patches = [create_patches(imgs[i]) for i in range(TRAINING_SIZE)] #list of images (=list windows (=list pixels))\n",
    "    \n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))] # j in range number of patch per image\n",
    "\n",
    "    return np.asarray(data)\n",
    "\n",
    "\n",
    "def load_data(tr_imgs, gt_imgs):\n",
    "        # Extract it into numpy arrays.\n",
    "        train_data = extract_data(tr_imgs) #shape: ((400/16)^2 * 100 = 62500, 16, 16, 3)\n",
    "        train_labels = extract_labels(gt_imgs)\n",
    "        return train_data, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = load_data(tr_imgs, gt_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.4664344 , 0.4538443 , 0.40348393],\n",
       "         [0.4553619 , 0.44548875, 0.41545388],\n",
       "         [0.5829659 , 0.57718354, 0.56541884],\n",
       "         ...,\n",
       "         [0.51327723, 0.50935566, 0.4952679 ],\n",
       "         [0.36658093, 0.36074167, 0.35272613],\n",
       "         [0.20534872, 0.1989317 , 0.19750558]],\n",
       "\n",
       "        [[0.6672187 , 0.65306   , 0.5964251 ],\n",
       "         [0.3769305 , 0.36548874, 0.3213362 ],\n",
       "         [0.5127776 , 0.5040528 , 0.48435277],\n",
       "         ...,\n",
       "         [0.46621838, 0.4622968 , 0.4513463 ],\n",
       "         [0.50618875, 0.50191814, 0.4876281 ],\n",
       "         [0.26438087, 0.2573933 , 0.25397104]],\n",
       "\n",
       "        [[0.85806227, 0.8421706 , 0.77971303],\n",
       "         [0.5202344 , 0.50722396, 0.45518234],\n",
       "         [0.43434626, 0.4240528 , 0.39023513],\n",
       "         ...,\n",
       "         [0.4198021 , 0.41588053, 0.4080374 ],\n",
       "         [0.5006678 , 0.49674627, 0.48349914],\n",
       "         [0.40398875, 0.3985698 , 0.388873  ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.09011009, 0.0949392 , 0.10026509],\n",
       "         [0.17014714, 0.16682543, 0.16641052],\n",
       "         [0.43369055, 0.42221802, 0.39191675],\n",
       "         ...,\n",
       "         [0.36929992, 0.36696193, 0.36379477],\n",
       "         [0.30937293, 0.30771813, 0.30658475],\n",
       "         [0.33654237, 0.34032142, 0.3364711 ]],\n",
       "\n",
       "        [[0.11677676, 0.12631175, 0.12536313],\n",
       "         [0.07058873, 0.0719729 , 0.08189204],\n",
       "         [0.2815337 , 0.27476704, 0.26172066],\n",
       "         ...,\n",
       "         [0.43988815, 0.43911877, 0.4328144 ],\n",
       "         [0.31762573, 0.31413943, 0.31326893],\n",
       "         [0.3208561 , 0.3214979 , 0.31921622]],\n",
       "\n",
       "        [[0.13931647, 0.15273196, 0.147985  ],\n",
       "         [0.0972554 , 0.10334545, 0.10699008],\n",
       "         [0.12937683, 0.12731606, 0.13152458],\n",
       "         ...,\n",
       "         [0.521666  , 0.5224652 , 0.51382285],\n",
       "         [0.38821396, 0.38629627, 0.38228852],\n",
       "         [0.30516982, 0.30267438, 0.3019613 ]]],\n",
       "\n",
       "\n",
       "       [[[0.15813999, 0.17469274, 0.16837715],\n",
       "         [0.12392206, 0.134718  , 0.13208812],\n",
       "         [0.07773405, 0.08037915, 0.08861705],\n",
       "         ...,\n",
       "         [0.614215  , 0.6165829 , 0.6063719 ],\n",
       "         [0.4588022 , 0.45845315, 0.45130813],\n",
       "         [0.3365398 , 0.3334738 , 0.33176267]],\n",
       "\n",
       "        [[0.17642951, 0.19603735, 0.18815315],\n",
       "         [0.14436021, 0.15861633, 0.15344906],\n",
       "         [0.10440071, 0.1117517 , 0.11371509],\n",
       "         ...,\n",
       "         [0.7053145 , 0.70919126, 0.69742656],\n",
       "         [0.54646444, 0.547684  , 0.5386213 ],\n",
       "         [0.40712804, 0.40563065, 0.40078226]],\n",
       "\n",
       "        [[0.1748609 , 0.19446874, 0.18501589],\n",
       "         [0.16318375, 0.18057711, 0.17384121],\n",
       "         [0.13058044, 0.14253992, 0.13852096],\n",
       "         ...,\n",
       "         [0.64570665, 0.64487755, 0.63311285],\n",
       "         [0.63901347, 0.64180166, 0.63117033],\n",
       "         [0.47871384, 0.47878507, 0.4708707 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.4351455 , 0.4121359 , 0.373719  ],\n",
       "         [0.4717827 , 0.4444372 , 0.40340403],\n",
       "         [0.3138702 , 0.29093033, 0.2597545 ],\n",
       "         ...,\n",
       "         [0.32092002, 0.2913314 , 0.2577674 ],\n",
       "         [0.39646918, 0.37182117, 0.33240297],\n",
       "         [0.6438102 , 0.6264218 , 0.5765008 ]],\n",
       "\n",
       "        [[0.4255948 , 0.40508074, 0.3653259 ],\n",
       "         [0.4537907 , 0.42953524, 0.3850905 ],\n",
       "         [0.42454937, 0.39847225, 0.35248542],\n",
       "         ...,\n",
       "         [0.42830563, 0.39869928, 0.3604708 ],\n",
       "         [0.46611634, 0.43864587, 0.39536244],\n",
       "         [0.5242586 , 0.49994007, 0.4510206 ]],\n",
       "\n",
       "        [[0.4146144 , 0.3995914 , 0.35905138],\n",
       "         [0.4334468 , 0.4153731 , 0.37167618],\n",
       "         [0.4520583 , 0.43362918, 0.38643968],\n",
       "         ...,\n",
       "         [0.51021355, 0.48523444, 0.4418891 ],\n",
       "         [0.502085  , 0.47662815, 0.42964244],\n",
       "         [0.4641915 , 0.44279844, 0.3924536 ]]]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/leobouraux/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1563/1562 [==============================] - 25s 16ms/step - loss: 1.0490 - acc: 0.3430 - val_loss: 0.7094 - val_acc: 0.3003\n",
      "Epoch 2/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 1.0374 - acc: 0.3644 - val_loss: 0.6645 - val_acc: 0.4487\n",
      "Epoch 3/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 1.0109 - acc: 0.4377 - val_loss: 0.6700 - val_acc: 0.4831\n",
      "Epoch 4/30\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.9743 - acc: 0.4984 - val_loss: 0.6338 - val_acc: 0.5308\n",
      "Epoch 5/30\n",
      "1563/1562 [==============================] - 21s 14ms/step - loss: 0.9571 - acc: 0.5177 - val_loss: 0.6320 - val_acc: 0.5401\n",
      "Epoch 6/30\n",
      "1563/1562 [==============================] - 22s 14ms/step - loss: 0.9548 - acc: 0.5175 - val_loss: 0.6116 - val_acc: 0.5723\n",
      "Epoch 7/30\n",
      "1563/1562 [==============================] - 15s 10ms/step - loss: 0.9511 - acc: 0.5253 - val_loss: 0.6710 - val_acc: 0.5010\n",
      "Epoch 8/30\n",
      "1563/1562 [==============================] - 15s 10ms/step - loss: 0.9495 - acc: 0.5270 - val_loss: 0.6788 - val_acc: 0.4765\n",
      "Epoch 9/30\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.9417 - acc: 0.5504 - val_loss: 0.7322 - val_acc: 0.4365\n",
      "Epoch 10/30\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.9272 - acc: 0.5853 - val_loss: 0.6295 - val_acc: 0.5818\n",
      "Epoch 11/30\n",
      "1563/1562 [==============================] - 15s 10ms/step - loss: 0.9098 - acc: 0.6115 - val_loss: 0.5609 - val_acc: 0.6839\n",
      "Epoch 12/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8952 - acc: 0.6295 - val_loss: 0.6656 - val_acc: 0.5736\n",
      "Epoch 13/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8895 - acc: 0.6367 - val_loss: 0.6390 - val_acc: 0.5970\n",
      "Epoch 14/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8895 - acc: 0.6358 - val_loss: 0.5947 - val_acc: 0.6457\n",
      "Epoch 15/30\n",
      "1563/1562 [==============================] - 18s 11ms/step - loss: 0.8852 - acc: 0.6423 - val_loss: 0.5601 - val_acc: 0.6856\n",
      "Epoch 16/30\n",
      "1563/1562 [==============================] - 16s 10ms/step - loss: 0.8836 - acc: 0.6425 - val_loss: 0.5828 - val_acc: 0.6626\n",
      "Epoch 17/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8863 - acc: 0.6398 - val_loss: 0.6811 - val_acc: 0.5666\n",
      "Epoch 18/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8835 - acc: 0.6418 - val_loss: 0.5854 - val_acc: 0.6575\n",
      "Epoch 19/30\n",
      "1563/1562 [==============================] - 17s 11ms/step - loss: 0.8856 - acc: 0.6424 - val_loss: 0.6007 - val_acc: 0.6398\n",
      "Epoch 20/30\n",
      "1563/1562 [==============================] - 22s 14ms/step - loss: 0.8828 - acc: 0.6405 - val_loss: 0.6515 - val_acc: 0.5946\n"
     ]
    }
   ],
   "source": [
    "def train(epochs=100, validation_split=0.1):\n",
    "\n",
    "    # Step 0: Shuffle samples\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(train_data)\n",
    "    # resetting the seed allows for an identical shuffling between y and x\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(train_labels)\n",
    "\n",
    "    # Step 1: Split into validation and training set     \n",
    "    split_index = int(len(train_data) * (1 - validation_split))\n",
    "    train_data_split = train_data[0:split_index]\n",
    "    validation_data_split = train_data[split_index:len(train_data)]\n",
    "    train_label_split = train_labels[0:split_index]\n",
    "    validation_label_split = train_labels[split_index:len(train_data)]\n",
    "\n",
    "    # Step 2: Give weights to classes\n",
    "    c_weight = {1: 3., \n",
    "                0: 1.}\n",
    "\n",
    "    # Step 3: Greate Generators\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        #rotation_range=180,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "        #rotation_range=180,\n",
    "        #horizontal_flip=True,\n",
    "        #vertical_flip=True)\n",
    "\n",
    "    train_generator = train_datagen.flow(train_data_split, train_label_split, batch_size=32)\n",
    "    validation_generator = validation_datagen.flow(validation_data_split, validation_label_split, batch_size=32)\n",
    "\n",
    "\n",
    "    # Step 4: Early stop\n",
    "    early_stop_callback = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0, \n",
    "                                        mode='max', restore_best_weights=True)\n",
    "\n",
    "    # Finally, train the model !\n",
    "    # Training\n",
    "    model.fit_generator(train_generator,\n",
    "                validation_data=validation_generator,\n",
    "                steps_per_epoch=len(train_data_split)/32,\n",
    "                epochs=epochs,\n",
    "                callbacks = [early_stop_callback],\n",
    "                class_weight=c_weight,\n",
    "                validation_steps=len(validation_data_split)/16)\n",
    "\n",
    "# Train model\n",
    "train(epochs=30, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
