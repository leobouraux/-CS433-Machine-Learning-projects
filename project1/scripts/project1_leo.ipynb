{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "Lets have a first see on the dataset. We observe that there is missing data all over tX. So we look at how many values are missing for each column, which corresponds to probilities to have a missing value in a column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible solution could be to remove columns with too many missing values, but these missing values may influence the final prediction. We therefore computed the probability of y given x=-999 and we saw in the results that if the proportion of -999 was very big, the conditional probability of y to be -1 is closer to 1 than the conditional probability of y to be 1 ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 0 has 15.245600000000001 % of missing values and we have:\n",
      "P(y=1|x=-999) = 7.438211680747233 % \n",
      "P(y=-1|x=-999)= 92.56178831925277 % \n",
      "\n",
      "Column 4 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 5 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 6 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 12 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 23 has 39.9652 % of missing values and we have:\n",
      "P(y=1|x=-999) = 25.514197351695977 % \n",
      "P(y=-1|x=-999)= 74.48580264830402 % \n",
      "\n",
      "Column 24 has 39.9652 % of missing values and we have:\n",
      "P(y=1|x=-999) = 25.514197351695977 % \n",
      "P(y=-1|x=-999)= 74.48580264830402 % \n",
      "\n",
      "Column 25 has 39.9652 % of missing values and we have:\n",
      "P(y=1|x=-999) = 25.514197351695977 % \n",
      "P(y=-1|x=-999)= 74.48580264830402 % \n",
      "\n",
      "Column 26 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 27 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n",
      "Column 28 has 70.9828 % of missing values and we have:\n",
      "P(y=1|x=-999) = 29.980220560473803 % \n",
      "P(y=-1|x=-999)= 70.0197794395262 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(tX.shape[1]):\n",
    "    indexes = tX.T[i]<-900\n",
    "    indexes0 = np.logical_and(indexes,y<=0)\n",
    "    indexes1 = np.logical_and(indexes,y>0)\n",
    "    tX_nine= tX[indexes]\n",
    "    tX_nine0 = tX[indexes0]\n",
    "    tX_nine1 = tX[indexes1]\n",
    "    count_of_mins = np.count_nonzero(tX.T[i] < -900)\n",
    "    if(count_of_mins > 0):\n",
    "        print(\"Column\", i, \"has\", count_of_mins /tX.shape[0] * 100, \"% of missing values and we have:\")\n",
    "        print(\"P(y=1|x=-999) =\", (tX_nine1.shape[0]/tX_nine.shape[0]*100), \"% \")\n",
    "        print(\"P(y=-1|x=-999)=\", (tX_nine0.shape[0]/tX_nine.shape[0] *100), \"% \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see column #22 contains values in [0,1,2,3]. Depending on this value, some columns will contain the missing value, because they can't be computed. For this reason, we want the columns to keep depending on the value of column #22 !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 30)\n",
      "(77544, 30)\n",
      "(72543, 30)\n",
      "Columns #22 contains 0, a row contains 10 columns with missing value.\n",
      "Columns #22 contains 1, a row contains 7 columns with missing value.\n",
      "Columns #22 contains 2 or 3, a row contains 0 columns with missing value.\n"
     ]
    }
   ],
   "source": [
    "train_type0 = tX[tX[:, 22] == 0]\n",
    "\n",
    "train_type1 = tX[tX[:, 22] == 1]\n",
    "train_type2 = tX[tX[:, 22] == 2]\n",
    "train_type3 = tX[tX[:, 22] == 3]\n",
    "train_type23 = np.append(train_type2, train_type3, axis=0)\n",
    "print(train_type0.shape)\n",
    "print(train_type1.shape)\n",
    "print(train_type23.shape)\n",
    "\n",
    "columns_to_keep0 = np.count_nonzero(train_type0[0] == -999, axis=0)\n",
    "columns_to_keep1 = np.count_nonzero(train_type1[0] == -999, axis=0)\n",
    "columns_to_keep23 = np.count_nonzero(train_type23[0] == -999, axis=0)\n",
    "\n",
    "print(\"Columns #22 contains 0, a row contains\", columns_to_keep0, \"columns with missing value.\")\n",
    "print(\"Columns #22 contains 1, a row contains\", columns_to_keep1, \"columns with missing value.\")\n",
    "print(\"Columns #22 contains 2 or 3, a row contains\", columns_to_keep23, \"columns with missing value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that column #0 has missing values that do not depend on column #22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col n°1 first 6 values, before preprocess: [ 138.47   160.937 -999.     143.905  175.864   89.744]\n",
      "col n°1 first 6 values, after preprocess:  [138.47       160.937      121.85852836 143.905      175.864\n",
      "  89.744     ]\n",
      "col n°2 first 6 values, after standardization:  [ 3.14910656e-01  7.40827026e-01 -1.00190288e-12  4.17944237e-01\n",
      "  1.02380444e+00 -6.08808624e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(y, tX0):\n",
    "    MISSING_DATA = -999\n",
    "    MISSING_DATA2 = 0\n",
    "    tX1 = np.delete(tX0, 22, axis=1)\n",
    "    \n",
    "    # replace missing data for all columns\n",
    "    x=tX1\n",
    "    print(\"col n°1 first 6 values, before preprocess:\", x[:, 0][:6])\n",
    "    replacement_values_by_col = np.ma.array(x, mask=[x==MISSING_DATA]).mean(axis=0)\n",
    "    replacement_values = np.tile(replacement_values_by_col, (len(x), 1))\n",
    "    x2= x.copy()\n",
    "    x2[x == MISSING_DATA] = replacement_values[x == MISSING_DATA]\n",
    "    # replace missing data for last column\n",
    "    last = tX[:,-1]\n",
    "    rvbc = np.ma.array(last, mask=[last==MISSING_DATA2]).mean(axis=0)\n",
    "    rv = np.tile(rvbc, (len(last), 1))\n",
    "    last2 = last.copy()\n",
    "    last2[last == MISSING_DATA2] = rv[last== MISSING_DATA2].flatten()\n",
    "    x2[:, -1] = last2\n",
    "    \n",
    "    \n",
    "    print(\"col n°1 first 6 values, after preprocess: \", x2[:, 0][:6])\n",
    "    standardized_x, _, _ = standardize(x2)\n",
    "    print(\"col n°2 first 6 values, after standardization: \", standardized_x[:, 0][:6], '\\n')\n",
    "    \n",
    "    return y, standardized_x\n",
    "\n",
    "y_train, tx_train = preprocess_data(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data2(y, tX0):\n",
    "    MISSING_DATA = -999\n",
    "    PJN = tX[:,22]\n",
    "\n",
    "    r_PNJ_0 = np.argwhere(PJN==0).flatten()\n",
    "    r_PNJ_1 = np.argwhere(PJN==1).flatten()\n",
    "    r_PNJ_23 = np.append(np.argwhere(PJN==2).flatten(), np.argwhere(PJN==3).flatten())\n",
    "    \n",
    "            \n",
    "    x0 = tX0[r_PNJ_0]\n",
    "    columns_to_del = np.argwhere(x0[0]==MISSING_DATA).flatten()\n",
    "    x00 = np.delete(x0, columns_to_del, axis=1)\n",
    "    x_0 = x00[:,:-2] #delete last column and PRI_jet_num column\n",
    "    x1 = tX0[r_PNJ_1]\n",
    "    columns_to_del = np.argwhere(x1[0]==MISSING_DATA).flatten()\n",
    "    x11 = np.delete(x1, columns_to_del, axis=1)\n",
    "    x_1 = np.delete(x11, -5, axis=1) #delete PRI_jet_num column\n",
    "    x23 = tX0[r_PNJ_23]\n",
    "    x_23 = np.delete(x23, 22, axis=1)\n",
    " \n",
    "    \n",
    "    # replace missing data for first column\n",
    "    replacement_values_by_col = np.ma.array(x_0, mask=[x_0==MISSING_DATA]).mean(axis=0)\n",
    "    replacement_values = np.tile(replacement_values_by_col, (len(x_0), 1))\n",
    "    x0= x_0.copy()\n",
    "    x0[x_0 == MISSING_DATA] = replacement_values[x_0 == MISSING_DATA]\n",
    "    replacement_values_by_col = np.ma.array(x_1, mask=[x_1==MISSING_DATA]).mean(axis=0)\n",
    "    replacement_values = np.tile(replacement_values_by_col, (len(x_1), 1))\n",
    "    x1= x_1.copy()\n",
    "    x1[x_1 == MISSING_DATA] = replacement_values[x_1 == MISSING_DATA]\n",
    "    replacement_values_by_col = np.ma.array(x_23, mask=[x_23==MISSING_DATA]).mean(axis=0)\n",
    "    replacement_values = np.tile(replacement_values_by_col, (len(x_23), 1))\n",
    "    x23= x_23.copy()\n",
    "    x23[x_23 == MISSING_DATA] = replacement_values[x_23 == MISSING_DATA]\n",
    "\n",
    "    \n",
    "    standardized_x0, _, _ = standardize(x0)\n",
    "    standardized_x1, _, _ = standardize(x1)\n",
    "    standardized_x23, _, _ = standardize(x23)    \n",
    "    \n",
    "    y_x0  = (y[r_PNJ_0], standardized_x0)\n",
    "    y_x1  = (y[r_PNJ_1], standardized_x1)\n",
    "    y_x23 = (y[r_PNJ_23], standardized_x23)\n",
    "    return y_x0, y_x1, y_x23\n",
    "\n",
    "y_x0, y_x1, y_x23 = preprocess_data2(y, tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913,) (99913, 18)\n",
      "(77544,) (77544, 22)\n",
      "(72543,) (72543, 29)\n"
     ]
    }
   ],
   "source": [
    "y_train0, tx_train0 = y_x0\n",
    "print(y_train0.shape, tx_train0.shape)\n",
    "y_train1, tx_train1 = y_x1\n",
    "print(y_train1.shape, tx_train1.shape)\n",
    "y_train23, tx_train23 = y_x23\n",
    "print(y_train23.shape, tx_train23.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares \n",
    "***Test for least squares method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8461cecbe477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx_train23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpx_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpx_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'split_data' is not defined"
     ]
    }
   ],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(tx_train23, y_train23, 0.95)\n",
    "px_tr = build_poly(x_tr, 1)\n",
    "px_te = build_poly(x_te, 1)\n",
    "print(y_tr.shape)\n",
    "print(px_tr.shape)\n",
    "loss_LS, w_LS = least_squares(y_tr, px_tr)\n",
    "\n",
    "y_validation_tr = predict_labels(w_LS, px_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for LS (S train):', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w_LS, px_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for LS (S test):', accuracy_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Best degree choice with least squares method using build poly***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deg: 6     (175, 175) (175,)\n",
      "Accuracy for LS (S test): 0.8023704520396913\n",
      "Deg: 7     (204, 204) (204,)\n",
      "Accuracy for LS (S test): 0.8106394707828004\n",
      "Deg: 8     (233, 233) (233,)\n",
      "Accuracy for LS (S test): 0.8211135611907387\n",
      "Deg: 9     (262, 262) (262,)\n",
      "Accuracy for LS (S test): 0.8252480705622933\n",
      "Deg: 10     (291, 291) (291,)\n",
      "Accuracy for LS (S test): 0.8257993384785005\n",
      "Deg: 11     (320, 320) (320,)\n",
      "Accuracy for LS (S test): 0.8235942668136714\n",
      "Deg: 12     (349, 349) (349,)\n",
      "Accuracy for LS (S test): 0.8252480705622933\n",
      "Deg: 13     (378, 378) (378,)\n",
      "Accuracy for LS (S test): 0.8244211686879823\n",
      "Deg: 14     (407, 407) (407,)\n",
      "Accuracy for LS (S test): 0.5350055126791621\n",
      "\n",
      "The best degree for least squares is:  10\n"
     ]
    }
   ],
   "source": [
    "def find_best_degree_LS(x_tr, x_te, y_tr, y_te, degrees):\n",
    "    accuracy = []\n",
    "    for deg in degrees:\n",
    "        print('Deg:', deg, end = '     ')\n",
    "        px_tr = build_poly(x_tr, deg)\n",
    "        px_te = build_poly(x_te, deg)\n",
    "        \n",
    "        loss, w = least_squares(y_tr, px_tr)\n",
    "        \n",
    "        y_validation_te = predict_labels(w, px_te)\n",
    "        accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "        print('Accuracy for LS (S test):', accuracy_te)\n",
    "        accuracy.append(accuracy_te)\n",
    "            \n",
    "    index = np.nanargmax(accuracy)\n",
    "    best_deg = degrees[index]\n",
    "    return best_deg\n",
    "\n",
    "best_deg = find_best_degree_LS(x_tr, x_te, y_tr, y_te, range(6,15))\n",
    "print('\\nThe best degree for least squares is: ', best_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827228921312325"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.850480384307446*tx_train0.shape[0]+0.798607529654461*tx_train1.shape[0] + 0.8257993384785005*tx_train23.shape[0]\n",
    "a/tx_train.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST = 0.8218 en faisant le bail moyen sur la derniere colone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Best accuracy with least squares***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(349, 349) (349,)\n",
      "Accuracy for LS (S train): 0.818858947368421\n",
      "Accuracy for LS (S test): 0.818\n"
     ]
    }
   ],
   "source": [
    "##### Least squares test using build-poly with best-degree\n",
    "px_tr = build_poly(x_tr, best_deg)\n",
    "px_te = build_poly(x_te, best_deg)\n",
    "loss_LSp, w_LSp = least_squares(y_tr, px_tr)\n",
    "\n",
    "y_validation_tr = predict_labels(w_LSp, px_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for LS (S train):', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w_LSp, px_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for LS (S test):', accuracy_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "***Test for least squares with gradient descent***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for GD (S train): 0.740235\n",
      "Accuracy for GD (S test): 0.73864\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.1\n",
    "degree = 1\n",
    "max_iters = 200\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tx_train, y_train, 0.8)\n",
    "px_tr = build_poly(x_tr, degree)\n",
    "px_te = build_poly(x_te, degree)\n",
    "\n",
    "initial_w = np.random.normal(0, 1, px_tr.shape[1])\n",
    "loss_GD, w_GD = least_squares_GD(y_tr, px_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "y_validation_tr = predict_labels(w_GD, px_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for GD (S train):', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w_GD, px_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for GD (S test):', accuracy_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7484849890960877"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.8007806635640294*tx_train0.shape[0]+0.7047520794377459*tx_train1.shape[0] + 0.7232062857536702*tx_train23.shape[0]\n",
    "a/tx_train.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Best gamma choice with least squares method for linear regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def find_best_gamma_GD(x, y, deg, gammas):\n",
    "    max_iters = 300\n",
    "    accuracy = []\n",
    "    print('Degree:', deg)\n",
    "\n",
    "    for gam in gammas:\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, 0.8)\n",
    "        px_tr = build_poly(x_tr, deg)\n",
    "        px_te = build_poly(x_te, deg)\n",
    "        initial_w = np.random.normal(0, 1, px_tr.shape[1])\n",
    "        \n",
    "        loss_GD, w_GD = least_squares_GD(y_tr, px_tr, initial_w, max_iters, gam)\n",
    "        \n",
    "        y_validation_te = predict_labels(w_GD, px_te)\n",
    "        accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "        print('Gamma: ', gam, '    accuracy for GD (S test):', accuracy_te)\n",
    "        accuracy.append(accuracy_te)\n",
    "            \n",
    "    index = np.nanargmax(accuracy)\n",
    "    best_lambda = gammas[index]\n",
    "    return best_lambda, np.max(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 1\n",
      "Gamma:  0.1     accuracy for GD (S test): 0.72742\n",
      "Gamma:  0.14444444444444446     accuracy for GD (S test): 0.73648\n",
      "Gamma:  0.18888888888888888     accuracy for GD (S test): 0.74008\n",
      "Gamma:  0.23333333333333334     accuracy for GD (S test): 0.74128\n",
      "Gamma:  0.2777777777777778     accuracy for GD (S test): 0.74274\n",
      "Gamma:  0.32222222222222224     accuracy for GD (S test): 0.74334\n",
      "Gamma:  0.3666666666666667     accuracy for GD (S test): 0.74372\n",
      "Gamma:  0.4111111111111111     accuracy for GD (S test): 0.3655\n",
      "Gamma:  0.4555555555555556     accuracy for GD (S test): 0.3655\n",
      "Gamma:  0.5     accuracy for GD (S test): 0.3655\n",
      "\n",
      "The best gamma for GD with degree 1 is:  0.3666666666666667  with an accuracy of:  0.74372\n"
     ]
    }
   ],
   "source": [
    "best_lambda, best_acc = find_best_gamma_GD(tx_train, y_train, 1, np.linspace(0.1, 0.5, 10))\n",
    "print('\\nThe best gamma for GD with degree 1 is: ', best_lambda, ' with an accuracy of: ', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that using a polynomial regression (using a degree greater than 2 with buildpoly) the GD had difficulties to converge.\n",
    "We assumed that this was a difficult task for GD because the gamma used is the same for all degrees of our polynomial. However, a gradient is optimal for a fixed order of magnitude not for the entire polynomial. So we decided to keep a linear regression in the case of GD and SGD, obviously it is not optimal compared to the other functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "***Test for least squares with stochastic gradient descent***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for SGD (S train): 0.72228\n",
      "Accuracy for SGD (S test): 0.71988\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "degree = 1\n",
    "max_iters = 200\n",
    "\n",
    "x_tr, x_te, y_tr, y_te = split_data(tx_train, y_train, 0.8)\n",
    "px_tr = build_poly(x_tr, degree)\n",
    "px_te = build_poly(x_te, degree)\n",
    "\n",
    "initial_w = np.zeros(px_tr.shape[1])\n",
    "loss_SGD, w_SGD = least_squares_SGD(y_tr, px_tr, initial_w, max_iters, gamma)\n",
    "\n",
    "\n",
    "y_validation_tr = predict_labels(w_SGD, px_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for SGD (S train):', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w_SGD, px_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for SGD (S test):', accuracy_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def find_best_gamma_SGD(x, y, deg, gammas):\n",
    "    max_iters = 300\n",
    "    accuracy = []\n",
    "    print('Degree:', deg)\n",
    "\n",
    "    for gam in gammas:\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, 0.8)\n",
    "        px_tr = build_poly(x_tr, deg)\n",
    "        px_te = build_poly(x_te, deg)\n",
    "        initial_w = np.zeros(px_tr.shape[1])\n",
    "        \n",
    "        loss_SGD, w_SGD = least_squares_SGD(y_tr, px_tr, initial_w, max_iters, gam)\n",
    "        \n",
    "        y_validation_te = predict_labels(w_SGD, px_te)\n",
    "        accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "        print('Gamma: ', gam, '    accuracy for SGD (S test):', accuracy_te)\n",
    "        accuracy.append(accuracy_te)\n",
    "            \n",
    "    index = np.nanargmax(accuracy)\n",
    "    best_lambda = gammas[index]\n",
    "    return best_lambda, np.max(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 1\n",
      "Gamma:  0.0005     accuracy for SGD (S test): 0.70672\n",
      "Gamma:  0.001     accuracy for SGD (S test): 0.71024\n",
      "Gamma:  0.0015     accuracy for SGD (S test): 0.71104\n",
      "Gamma:  0.002     accuracy for SGD (S test): 0.71186\n",
      "Gamma:  0.0025     accuracy for SGD (S test): 0.71196\n",
      "Gamma:  0.003     accuracy for SGD (S test): 0.71178\n",
      "Gamma:  0.0035     accuracy for SGD (S test): 0.71174\n",
      "Gamma:  0.004     accuracy for SGD (S test): 0.7109\n",
      "Gamma:  0.0045000000000000005     accuracy for SGD (S test): 0.71054\n",
      "Gamma:  0.005     accuracy for SGD (S test): 0.70992\n",
      "\n",
      "The best gamma for GD with degree 1 is:  0.0025  with an accuracy of:  0.71196\n"
     ]
    }
   ],
   "source": [
    "best_lambda, best_acc = find_best_gamma_SGD(tx_train, y_train, 1, np.linspace(0.0005, 0.005, 10 ))\n",
    "print('\\nThe best gamma for SGD with degree 1 is: ', best_lambda, ' with an accuracy of: ', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,) (250000, 28)\n",
      "\n",
      "Lam: 0.0001\n",
      "Accuracy for Ridge (S test): 0.80288\n",
      "\n",
      "Lam: 0.0002\n",
      "Accuracy for Ridge (S test): 0.80352\n",
      "\n",
      "Lam: 0.0003\n",
      "Accuracy for Ridge (S test): 0.79652\n",
      "0.0002\n"
     ]
    }
   ],
   "source": [
    "def find_best_param_ridge(y, x, lambdas):\n",
    "    accuracy = []\n",
    "    for lam in lambdas:\n",
    "        print('\\nLam:', lam)\n",
    "        x_tr, x_te, y_tr, y_te = split_data(x, y, 0.8)\n",
    "        px_tr = build_poly(x_tr, 7)\n",
    "        px_te = build_poly(x_te, 7)\n",
    "        loss, w = ridge_regression(y_tr, px_tr, lam)\n",
    "    \n",
    "        y_validation_te = predict_labels(w, px_te)\n",
    "        #print(y_validation_te.shape, y_te.shape)\n",
    "        accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "        print('Accuracy for Ridge (S test):', accuracy_te)\n",
    "        accuracy.append(accuracy_te)\n",
    "        \n",
    "    index = np.nanargmax(accuracy)\n",
    "    best_lam = lambdas[index]\n",
    "        \n",
    "    return best_lam\n",
    "\n",
    "print(y_train.shape, tx_train.shape)\n",
    "best_lambda = find_best_param_ridge(y_train, tx_train, [1e-4, 2e-4, 3e-4])\n",
    "\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train:\n",
    "    x_test = x[k_indices[k]]\n",
    "    y_test = y[k_indices[k]]\n",
    "    \n",
    "    A = np.ones(y.shape, dtype = bool)\n",
    "    A[:]=True\n",
    "    A[k_indices[k]] = False\n",
    "    \n",
    "    x_train=x[A]\n",
    "    y_train = y[A]\n",
    "    \n",
    "    # form data with polynomial degree:\n",
    "    train_poly = build_poly(x_train, degree)\n",
    "    test_poly = build_poly(x_test, degree)\n",
    "    \n",
    "    # ridge regression:\n",
    "    loss, w = ridge_regression(y_train, train_poly, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data:\n",
    "    e_tr = y_train-train_poly@w\n",
    "    e_te = y_test-test_poly@w\n",
    "    loss_tr = np.mean(e_tr**2)\n",
    "    loss_te = np.mean(e_te**2)\n",
    "    return loss_tr, loss_te\n",
    "\n",
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda =  1e-06 k_fold =  0\n",
      "lambda =  1e-06 k_fold =  1\n",
      "lambda =  1e-06 k_fold =  2\n",
      "lambda =  1e-06 k_fold =  3\n",
      "lambda =  1e-06 k_fold =  4\n",
      "lambda =  1e-06 k_fold =  5\n",
      "lambda =  1e-06 k_fold =  6\n",
      "lambda =  1e-06 k_fold =  7\n",
      "lambda =  1e-06 k_fold =  8\n",
      "lambda =  1e-06 k_fold =  9\n",
      "lambda =  5.994842503189409e-06 k_fold =  0\n",
      "lambda =  5.994842503189409e-06 k_fold =  1\n",
      "lambda =  5.994842503189409e-06 k_fold =  2\n",
      "lambda =  5.994842503189409e-06 k_fold =  3\n",
      "lambda =  5.994842503189409e-06 k_fold =  4\n",
      "lambda =  5.994842503189409e-06 k_fold =  5\n",
      "lambda =  5.994842503189409e-06 k_fold =  6\n",
      "lambda =  5.994842503189409e-06 k_fold =  7\n",
      "lambda =  5.994842503189409e-06 k_fold =  8\n",
      "lambda =  5.994842503189409e-06 k_fold =  9\n",
      "lambda =  3.5938136638046256e-05 k_fold =  0\n",
      "lambda =  3.5938136638046256e-05 k_fold =  1\n",
      "lambda =  3.5938136638046256e-05 k_fold =  2\n",
      "lambda =  3.5938136638046256e-05 k_fold =  3\n",
      "lambda =  3.5938136638046256e-05 k_fold =  4\n",
      "lambda =  3.5938136638046256e-05 k_fold =  5\n",
      "lambda =  3.5938136638046256e-05 k_fold =  6\n",
      "lambda =  3.5938136638046256e-05 k_fold =  7\n",
      "lambda =  3.5938136638046256e-05 k_fold =  8\n",
      "lambda =  3.5938136638046256e-05 k_fold =  9\n",
      "lambda =  0.00021544346900318845 k_fold =  0\n",
      "lambda =  0.00021544346900318845 k_fold =  1\n",
      "lambda =  0.00021544346900318845 k_fold =  2\n",
      "lambda =  0.00021544346900318845 k_fold =  3\n",
      "lambda =  0.00021544346900318845 k_fold =  4\n",
      "lambda =  0.00021544346900318845 k_fold =  5\n",
      "lambda =  0.00021544346900318845 k_fold =  6\n",
      "lambda =  0.00021544346900318845 k_fold =  7\n",
      "lambda =  0.00021544346900318845 k_fold =  8\n",
      "lambda =  0.00021544346900318845 k_fold =  9\n",
      "lambda =  0.001291549665014884 k_fold =  0\n",
      "lambda =  0.001291549665014884 k_fold =  1\n",
      "lambda =  0.001291549665014884 k_fold =  2\n",
      "lambda =  0.001291549665014884 k_fold =  3\n",
      "lambda =  0.001291549665014884 k_fold =  4\n",
      "lambda =  0.001291549665014884 k_fold =  5\n",
      "lambda =  0.001291549665014884 k_fold =  6\n",
      "lambda =  0.001291549665014884 k_fold =  7\n",
      "lambda =  0.001291549665014884 k_fold =  8\n",
      "lambda =  0.001291549665014884 k_fold =  9\n",
      "lambda =  0.007742636826811269 k_fold =  0\n",
      "lambda =  0.007742636826811269 k_fold =  1\n",
      "lambda =  0.007742636826811269 k_fold =  2\n",
      "lambda =  0.007742636826811269 k_fold =  3\n",
      "lambda =  0.007742636826811269 k_fold =  4\n",
      "lambda =  0.007742636826811269 k_fold =  5\n",
      "lambda =  0.007742636826811269 k_fold =  6\n",
      "lambda =  0.007742636826811269 k_fold =  7\n",
      "lambda =  0.007742636826811269 k_fold =  8\n",
      "lambda =  0.007742636826811269 k_fold =  9\n",
      "lambda =  0.04641588833612782 k_fold =  0\n",
      "lambda =  0.04641588833612782 k_fold =  1\n",
      "lambda =  0.04641588833612782 k_fold =  2\n",
      "lambda =  0.04641588833612782 k_fold =  3\n",
      "lambda =  0.04641588833612782 k_fold =  4\n",
      "lambda =  0.04641588833612782 k_fold =  5\n",
      "lambda =  0.04641588833612782 k_fold =  6\n",
      "lambda =  0.04641588833612782 k_fold =  7\n",
      "lambda =  0.04641588833612782 k_fold =  8\n",
      "lambda =  0.04641588833612782 k_fold =  9\n",
      "lambda =  0.2782559402207126 k_fold =  0\n",
      "lambda =  0.2782559402207126 k_fold =  1\n",
      "lambda =  0.2782559402207126 k_fold =  2\n",
      "lambda =  0.2782559402207126 k_fold =  3\n",
      "lambda =  0.2782559402207126 k_fold =  4\n",
      "lambda =  0.2782559402207126 k_fold =  5\n",
      "lambda =  0.2782559402207126 k_fold =  6\n",
      "lambda =  0.2782559402207126 k_fold =  7\n",
      "lambda =  0.2782559402207126 k_fold =  8\n",
      "lambda =  0.2782559402207126 k_fold =  9\n",
      "lambda =  1.6681005372000592 k_fold =  0\n",
      "lambda =  1.6681005372000592 k_fold =  1\n",
      "lambda =  1.6681005372000592 k_fold =  2\n",
      "lambda =  1.6681005372000592 k_fold =  3\n",
      "lambda =  1.6681005372000592 k_fold =  4\n",
      "lambda =  1.6681005372000592 k_fold =  5\n",
      "lambda =  1.6681005372000592 k_fold =  6\n",
      "lambda =  1.6681005372000592 k_fold =  7\n",
      "lambda =  1.6681005372000592 k_fold =  8\n",
      "lambda =  1.6681005372000592 k_fold =  9\n",
      "lambda =  10.0 k_fold =  0\n",
      "lambda =  10.0 k_fold =  1\n",
      "lambda =  10.0 k_fold =  2\n",
      "lambda =  10.0 k_fold =  3\n",
      "lambda =  10.0 k_fold =  4\n",
      "lambda =  10.0 k_fold =  5\n",
      "lambda =  10.0 k_fold =  6\n",
      "lambda =  10.0 k_fold =  7\n",
      "lambda =  10.0 k_fold =  8\n",
      "lambda =  10.0 k_fold =  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 7\n",
    "    k_fold = 10\n",
    "    lambdas = np.logspace(-6, 1, 10)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_train, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr_total = []\n",
    "    rmse_te_total = []\n",
    "    \n",
    "    # cross validation:\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        for k in range (k_fold):\n",
    "            print(\"lambda = \", lambda_, \"k_fold = \", k)\n",
    "            rmse_tr_tmp, rmse_te_tmp = cross_validation(y_train, tx_train, k_indices, k, lambda_, degree)\n",
    "            rmse_tr.append(np.sqrt(rmse_tr_tmp))\n",
    "            rmse_te.append(np.sqrt(rmse_te_tmp))\n",
    "        rmse_tr_total.append(np.mean(rmse_tr))\n",
    "        rmse_te_total.append(np.mean(rmse_te))\n",
    "        \n",
    "    cross_validation_visualization(lambdas, rmse_tr_total, rmse_te_total)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ridge : \n",
      " 325.1758083976528\n",
      "Weight ridge : \n",
      " [-2.83113814e-01  3.88289207e-01 -3.92986244e-01 -3.44771223e-01\n",
      "  1.17709405e-01  8.02411312e-02 -3.81000126e-02 -9.94487711e-02\n",
      "  1.83944582e-01 -5.03786693e-02 -1.35537824e-01 -1.90012376e-01\n",
      " -1.11034414e-01  8.53860515e-02  2.18152001e-01  2.09699383e-02\n",
      " -1.70684104e-02  2.38451019e-01  3.65273027e-03  7.43522661e-03\n",
      "  5.46088499e-03 -2.59732532e-03 -2.29182371e-02  9.56376909e-02\n",
      " -8.10257612e-03 -1.48374365e-02  1.01321956e-01 -2.23551974e-03\n",
      "  9.76539719e-03 -1.57421822e-01  1.09703165e-02 -4.65765768e-02\n",
      "  6.59350194e-02 -2.42575008e-03  7.81911248e-02 -1.28795764e-02\n",
      " -1.23848298e-02 -2.11364352e-02 -3.21981546e-02  1.02621065e-01\n",
      "  9.57267001e-03 -8.75961137e-02 -8.70448164e-02  9.09791055e-03\n",
      " -8.30882110e-03 -9.56094646e-02 -4.13120489e-02  1.06389388e-02\n",
      "  3.42979933e-02 -8.45899446e-03 -7.09248093e-02 -3.03090373e-02\n",
      "  1.06138194e-01  3.20917924e-02 -1.29096671e-02  5.47298704e-02\n",
      " -4.70129787e-02 -7.20539858e-02  9.38851704e-02  3.83990967e-02\n",
      " -6.86270483e-02 -3.65679742e-02 -3.27200672e-02  5.17415290e-04\n",
      "  1.35375998e-02  2.23725823e-02 -1.89473415e-02 -3.28514802e-02\n",
      "  1.85830331e-01  7.36976328e-03  1.40822975e-02 -2.31335523e-02\n",
      " -1.98698455e-02  4.96853075e-02 -1.55853453e-02  8.16652440e-03\n",
      " -3.57170314e-02 -2.10243205e-02  1.95718174e-02  3.27882536e-02\n",
      " -2.00239008e-03  1.48133183e-02 -9.48812541e-05 -1.34869672e-04\n",
      " -1.18657397e-02  3.18082166e-02 -2.80801626e-02 -6.31165896e-03\n",
      "  3.09693764e-02  7.28572766e-03  5.69382860e-03  4.65890992e-04\n",
      " -1.53757376e-02 -4.49198319e-03  1.23631532e-02  4.92836353e-03\n",
      " -6.18466551e-03  4.28177696e-02 -6.05100688e-04 -2.79527846e-02\n",
      "  1.20282822e-03 -1.11961311e-02 -1.16940654e-02 -7.25756213e-03\n",
      "  1.21130160e-02  1.23755235e-02 -7.80823868e-04 -1.22042247e-02\n",
      " -4.10734085e-03 -1.26542262e-02  2.23446894e-04 -1.14473339e-03\n",
      "  1.01842853e-02 -4.12772336e-03  3.23671701e-03  4.42655672e-04\n",
      " -6.64783378e-03  2.63546316e-03 -4.81979920e-04  3.98102386e-06\n",
      "  4.31318609e-03  3.76335834e-04 -2.34454510e-03 -3.68589554e-04\n",
      "  7.94500750e-02 -9.84161322e-03 -3.59822812e-05  1.18379158e-02\n",
      "  1.83953834e-02  1.16875137e-03  9.13557329e-03 -7.31014920e-03\n",
      " -1.77142134e-03  2.17837423e-02 -2.56738144e-04  1.92275583e-03\n",
      "  9.89539573e-04 -5.23009440e-03 -2.24274474e-05  1.87241205e-05\n",
      "  2.58576537e-03  2.20359277e-04 -1.65178551e-04 -1.41712217e-05\n",
      "  6.49446957e-04 -9.93149856e-04  1.95113200e-05 -7.00468733e-06\n",
      "  2.71378849e-03 -1.42772239e-05  1.89787242e-04  1.33012887e-05\n",
      "  6.67883864e-03 -5.01271433e-03  3.33789555e-06  4.83920253e-03\n",
      "  6.25689418e-04 -5.64638741e-05  2.66271946e-03  1.68651756e-03\n",
      "  1.16248293e-04 -3.47140377e-03  2.11194643e-05 -1.34247152e-04\n",
      " -8.68141276e-05  1.40605510e-03  8.56420827e-07  1.55376376e-06\n",
      " -6.24221850e-04 -4.17905366e-06  3.11203714e-06  1.70078130e-07\n",
      " -2.32355367e-05  8.86119647e-05 -3.01510893e-07  2.84491440e-07\n",
      " -7.40084047e-04  2.02741360e-07 -5.60963526e-06 -1.84260815e-07\n",
      " -8.54941959e-02  1.44277639e-03 -6.46964883e-08 -1.65998889e-03\n",
      " -4.17724060e-03  1.02240242e-06 -1.61612573e-03  1.49255389e-03\n",
      " -2.80348291e-06 -5.23455478e-03 -1.69703957e-07  3.42116010e-06\n",
      " -6.60035639e-05  6.56316694e-04 -1.14216035e-08  3.62603271e-07\n",
      " -1.55736467e-04]\n"
     ]
    }
   ],
   "source": [
    "##### Ridge test\n",
    "x_tr, x_te, y_tr, y_te = split_data(tx_train, y_train, 0.72)\n",
    "px_tr = build_poly(x_tr, 7)\n",
    "px_te = build_poly(x_te, 7)\n",
    "loss, w = ridge_regression(y_tr, px_tr, 0.0002)\n",
    "\n",
    "print(\"Loss ridge : \\n\", loss)\n",
    "print(\"Weight ridge : \\n\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Ridge (S train): 0.8004833333333333\n",
      "Accuracy for Ridge (S test): 0.8005857142857142\n"
     ]
    }
   ],
   "source": [
    "y_validation_tr = predict_labels(w, px_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for Ridge (S train):', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w, px_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for Ridge (S test):', accuracy_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Logistic regression***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gam: 5e-06  Lam: 0\n",
      " Gam: 5e-06  Lam: 0.001\n",
      " Gam: 5e-06  Lam: 1e-06\n",
      " Gam: 5e-05  Lam: 0\n",
      " Gam: 5e-05  Lam: 0.001\n",
      " Gam: 5e-05  Lam: 1e-06\n",
      " Gam: 0.0005  Lam: 0\n",
      " Gam: 0.0005  Lam: 0.001\n",
      " Gam: 0.0005  Lam: 1e-06\n",
      " Gam: 0.005  Lam: 0\n",
      " Gam: 0.005  Lam: 0.001\n",
      " Gam: 0.005  Lam: 1e-06\n"
     ]
    }
   ],
   "source": [
    "def find_best_param_logreg(y, x, gammas, lambdas, seed, max_iters, batch_size):\n",
    "    best_losses = []\n",
    "    best_gammas = []\n",
    "    best_lambdas = []\n",
    "    for gam in gammas:\n",
    "        loss_gam_deg = []\n",
    "        for lam in lambdas:\n",
    "            print( ' Gam:', gam, ' Lam:', lam)\n",
    "            initial_w = np.zeros(x_tr.shape[1], dtype=np.float64)\n",
    "            loss, w = logistic_regression(y, x, initial_w, batch_size, max_iters, gam, lam)\n",
    "            if np.isnan(loss):\n",
    "                loss_gam_deg.append(10^999)\n",
    "            else:\n",
    "                loss_gam_deg.append(loss)\n",
    "        \n",
    "        index = np.nanargmin(loss_gam_deg)\n",
    "        \n",
    "        best_lam_for_gam = lambdas[index]\n",
    "        best_loss_for_gam = loss_gam_deg[index]\n",
    "        \n",
    "        best_losses.append(best_loss_for_gam)\n",
    "        best_lambdas.append(best_lam_for_gam)\n",
    "        \n",
    "    best_index = np.nanargmin(best_losses)\n",
    "    best_gam = gammas[best_index]\n",
    "    best_lam = best_lambdas[best_index]\n",
    "\n",
    "    return best_gam, best_lam\n",
    "\n",
    "lambdas=[0, 1e-3, 1e-6]\n",
    "gammas=[5e-6, 5e-5, 5e-4, 5e-3]\n",
    "best_gamma, best_lam = find_best_param_logreg(y_tr, x_tr, gammas, lambdas, 1, 24, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-05 0\n"
     ]
    }
   ],
   "source": [
    "print(best_gamma, best_lam)\n",
    "initial_w = np.zeros(x_tr.shape[1], dtype=np.float64)\n",
    "loss_log, w_log = logistic_regression(y_tr, x_tr, initial_w, 1, 10, 5e-6, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for regularized logistic regression: 0.710416\n",
      "Accuracy for regularized logistic regression: 0.710928\n"
     ]
    }
   ],
   "source": [
    "y_validation_tr = predict_labels(w_log, x_tr)\n",
    "accuracy_tr = sum(y_validation_tr == y_tr)/len(y_tr)\n",
    "print('Accuracy for regularized logistic regression:', accuracy_tr)\n",
    "\n",
    "y_validation_te = predict_labels(w_log, x_te)\n",
    "accuracy_te = sum(y_validation_te == y_te)/len(y_te)\n",
    "print('Accuracy for regularized logistic regression:', accuracy_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "47.575\n",
      "col n°1 first 6 values, before preprocess: [-999.     106.398  117.794  135.861   74.159   95.709]\n",
      "col n°1 first 6 values, after preprocess:  [121.87172934 106.398      117.794      135.861       74.159\n",
      "  95.709     ]\n",
      "col n°2 first 6 values, after standardization:  [-2.25397884e-12 -2.95946335e-01 -7.79895413e-02  2.67554982e-01\n",
      " -9.12540672e-01 -5.00381239e-01] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test, tx_test = preprocess_data(_, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'output.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "px_te = build_poly(tx_test, 7)\n",
    "y_pred = predict_labels(w_LSp, px_te)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
