{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Least squares\n",
    "def least_squares(y, tx):\n",
    "    N = y.shape[0]\n",
    "    a = tx.T@tx\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve((a, b))\n",
    "    e = y - tx@w\n",
    "    loss = e.T@e / (2*N)\n",
    "    return loss, w\n",
    "\n",
    "## Least squares Gradient Descent                     \n",
    "def compute_gradient(y, tx, w):\n",
    "    N = y.shape\n",
    "    e = y - np.dot(tx,w)\n",
    "    return -np.dot(np.transpose(tx), e)/N\n",
    "\n",
    "def compute_mse(y, tx, w):\n",
    "    e = y - np.dot(tx,w)\n",
    "    return np.sum(e**2)/(2*y.shape[0])\n",
    "                        \n",
    "def least_squares_GD(y, tx, w0, max_iters, gamma):\n",
    "    w = w0\n",
    "    for n_iter in range(max_iters):\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        loss = compute_mse(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w\n",
    "                        \n",
    "## Least squares Stochastic Gradient Descent\n",
    "def least_squares_SGD(y, tx, w0, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    w = w0\n",
    "    for n_iter in range(max_iters):\n",
    "        for y_batch, tx_batch in batch_iter(y, tx, batch_size=batch_size, num_batches=1):\n",
    "            # compute a stochastic gradient and loss\n",
    "            grad, _ = compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "            # update w through the stochastic gradient update\n",
    "            w = w - gamma * grad\n",
    "            # calculate loss\n",
    "            loss = compute_mse(y, tx, w)\n",
    "        print(\"SGD({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return loss, w\n",
    "                        \n",
    "## Ridge regression\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    a = tx.T@tx +  (2 * tx.shape[0] * lambda_ * np.eye(tx.shape[1]))\n",
    "    b = tx.T@y\n",
    "    w = np.linalg.solve(a, b)\n",
    "    e = y - tx@w\n",
    "    loss = e.T@e / (2*N)\n",
    "    return loss, w\n",
    "\n",
    "## Logistic regression\n",
    "def log_reg(y, tx, w0, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        A = sigmoid(tx@w)\n",
    "        loss = y.T@(np.log(A)) + (1-y).T@np.log(1-A)\n",
    "        grad = tx.T@(A - y)\n",
    "        w = - grad * gamma                \n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            return loss[-1], w\n",
    "    \n",
    "## Regularized logistic regression\n",
    "def reg_log_reg(y, tx, w0, max_iters, gamma):\n",
    "    threshold = 1e-8\n",
    "    loss = []\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        A = sigmoid(tx@w)\n",
    "        loss = y.T@(np.log(A)) + (1-y).T@np.log(1-A) + lambda_*w.T@w\n",
    "        gradient = tx.T@(A - y) + 2*lambda_*w\n",
    "        w = w - gradient * gamma\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            return loss[-1], w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
